programs:
  # Like bio, but with tracepoints. It may catch more than just bio, especially
  # if you have RAID, because requests are getting split and remapped.
  #
  # See:
  # * https://github.com/iovisor/bcc/issues/826
  - name: bio
    metrics:
      histograms:
        - name: bio_latency_seconds
          help: Block IO latency histogram
          table: io_latency
          bucket_type: exp2
          bucket_min: 0
          bucket_max: 26
          bucket_multiplier: 0.000001 # microseconds to seconds
          labels:
            - name: device
              size: 4
              decoders:
                - name: majorminor
            - name: operation
              size: 4
              decoders:
                - name: uint
                - name: static_map
                  static_map:
                    1: read
                    2: write
            - name: bucket
              size: 8
              decoders:
                - name: uint
        - name: bio_size_bytes
          help: Block IO size histogram with kibibyte buckets
          table: io_size
          bucket_type: exp2
          bucket_min: 0
          bucket_max: 15
          bucket_multiplier: 1024 # kibibytes to bytes
          labels:
            - name: device
              size: 4
              decoders:
                - name: majorminor
            - name: operation
              size: 4
              decoders:
                - name: uint
                - name: static_map
                  static_map:
                    1: read
                    2: write
            - name: bucket
              size: 8
              decoders:
                - name: uint
    tracepoints:
      block:block_rq_issue: tracepoint__block__block_rq_issue
      block:block_rq_complete: tracepoint__block__block_rq_complete
    code: |
      #include <linux/blkdev.h>

      typedef struct disk_key {
          u32 dev;
          u8 op;
          u64 slot;
      } disk_key_t;

      // Max number of disks we expect to see on the host
      const u8 max_disks = 255;

      // 27 buckets for latency, max range is 33.6s .. 67.1s
      const u8 max_latency_slot = 26;

      // 16 buckets per disk in kib, max range is 16mib .. 32mib
      const u8 max_size_slot = 15;

      // Histograms to record latencies
      BPF_HISTOGRAM(io_latency, disk_key_t, (max_latency_slot + 2) * max_disks);

      // Histograms to record sizes
      BPF_HISTOGRAM(io_size, disk_key_t, (max_size_slot + 2) * max_disks);

      struct key_t {
          dev_t dev;
          sector_t sector;
      };

      struct val_t {
          u64 start;
          u64 bytes;
      };

      // Hash to temporily hold the start time of each bio request, max 10k in-flight by default
      BPF_HASH(start, struct key_t, struct val_t);

      // Generates function tracepoint__block__block_rq_issue
      TRACEPOINT_PROBE(block, block_rq_issue) {
          // blkid generates these and we're not interested in them
          if (args->dev == 0) {
              return 0;
          }

          struct key_t key = {};
          key.dev = args->dev;
          key.sector = args->sector;

          if (key.sector == -1) {
             key.sector = 0;
          }

          struct val_t val = {};
          val.start = bpf_ktime_get_ns();
          val.bytes = args->bytes;

          start.update(&key, &val);

          return 0;
      }

      // Generates function tracepoint__block__block_rq_complete
      TRACEPOINT_PROBE(block, block_rq_complete) {
          struct key_t key = {};
          key.dev = args->dev;
          key.sector = args->sector;

          if (key.sector == -1) {
             key.sector = 0;
          }

          struct val_t *valp = start.lookup(&key);
          if (valp == 0) {
              return 0; // missed issue
          }

          // Delta in microseconds
          u64 delta = (bpf_ktime_get_ns() - valp->start) / 1000;

          // Latency histogram key
          u64 latency_slot = bpf_log2l(delta);

          // Cap latency bucket at max value
          if (latency_slot > max_latency_slot) {
              latency_slot = max_latency_slot;
          }

          disk_key_t latency_key = {};
          latency_key.slot = latency_slot;
          latency_key.dev = new_encode_dev(args->dev);

          // Size in kibibytes
          u64 size_kib = valp->bytes / 1024;

          // Request size histogram key
          u64 size_slot = bpf_log2(size_kib);

          // Cap latency bucket at max value
          if (size_slot > max_size_slot) {
              size_slot = max_size_slot;
          }

          disk_key_t size_key = {};
          size_key.slot = size_slot;
          size_key.dev = new_encode_dev(args->dev);

          if (args->rwbs[0] == 'W' || args->rwbs[0] == 'S' || args->rwbs[0] == 'F' || args->rwbs[1] == 'W' || args->rwbs[1] == 'S' || args->rwbs[1] == 'F') {
              latency_key.op = 2;
              size_key.op    = 2;
          } else {
              latency_key.op = 1;
              size_key.op    = 1;
          }

          io_latency.increment(latency_key);
          io_size.increment(size_key);

          // Increment sum keys
          latency_key.slot = max_latency_slot + 1;
          io_latency.increment(latency_key, delta);
          size_key.slot = max_size_slot + 1;
          io_size.increment(size_key, size_kib);

          start.delete(&key);

          return 0;
      }

  # See:
  # * https://github.com/iovisor/bcc/blob/master/tools/runqlat.py
  # * https://github.com/iovisor/bcc/blob/master/tools/runqlat_example.txt
  - name: runqlat
    metrics:
      histograms:
        - name: run_queue_latency_seconds
          help: Run queue latency histogram
          table: run_queue_latencty
          bucket_type: exp2
          bucket_min: 0
          bucket_max: 26
          bucket_multiplier: 0.000001 # microseconds to seconds
          labels:
            - name: bucket
              size: 8
              decoders:
                - name: uint
    kprobes:
      ttwu_do_wakeup: trace_ttwu_do_wakeup
      wake_up_new_task: trace_wake_up_new_task
      finish_task_switch: trace_run
    code: |
      #include <linux/sched.h>

      // 27 buckets for latency, max range is 33.6s .. 67.1s
      const u8 max_latency_slot = 26;

      // Histograms to record latencies
      BPF_HISTOGRAM(run_queue_latencty, u64, max_latency_slot + 2);

      // Pid to enqueue time map
      BPF_HASH(start, u32);

      // Record enqueue timestamp
      static int trace_enqueue(u32 tgid, u32 pid) {
          if (tgid == 0 && pid == 0) {
              // Skip swapper kthread
              return 0;
          }

          u64 ts = bpf_ktime_get_ns();
          start.update(&pid, &ts);

          return 0;
      }

      int trace_wake_up_new_task(struct pt_regs *ctx, struct task_struct *p) {
          return trace_enqueue(p->tgid, p->pid);
      }

      int trace_ttwu_do_wakeup(struct pt_regs *ctx, void* rq, struct task_struct *p, int wake_flags) {
          return trace_enqueue(p->tgid, p->pid);
      }

      // Calculate latency
      int trace_run(struct pt_regs *ctx, struct task_struct *prev) {
          // Treat like an enqueue event and store timestamp
          if (prev->state == TASK_RUNNING) {
              trace_enqueue(prev->tgid, prev->pid);
          }

          u32 tgid = bpf_get_current_pid_tgid() >> 32;
          u32 pid = bpf_get_current_pid_tgid();

          // Fetch timestamp and calculate delta
          u64 *tsp = start.lookup(&pid);
          if (tsp == 0) {
              // Missed enqueue
              return 0;
          }

          // Latency in microseconds
          u64 latency_us = (bpf_ktime_get_ns() - *tsp) / 1000;

          // Latency histogram key
          u64 latency_slot = bpf_log2l(latency_us);

          // Cap latency bucket at max value
          if (latency_slot > max_latency_slot) {
              latency_slot = max_latency_slot;
          }

          // Increment bucket key
          run_queue_latencty.increment(latency_slot);

          // Increment sum key
          run_queue_latencty.increment(max_latency_slot + 1, latency_us);

          // Remove enqueued task
          start.delete(&pid);

          return 0;
      }

  # See:
  # * http://www.brendangregg.com/blog/2017-05-09/cpu-utilization-is-wrong.html
  - name: ipcstat
    metrics:
      counters:
        - name: cpu_instructions_total
          help: Instructions retired by CPUs
          table: instructions
          labels:
            - name: cpu
              size: 4
              decoders:
                - name: uint
        - name: cpu_cycles_total
          help: Cycles processed by CPUs
          table: cycles
          labels:
            - name: cpu
              size: 4
              decoders:
                - name: uint
    perf_events:
      - type: 0x0 # HARDWARE
        name: 0x1 # PERF_COUNT_HW_INSTRUCTIONS
        target: on_cpu_instruction
        sample_frequency: 99
      - type: 0x0 # HARDWARE
        name: 0x0 # PERF_COUNT_HW_CPU_CYCLES
        target: on_cpu_cycle
        sample_frequency: 99
    code: |
      #include <linux/ptrace.h>
      #include <uapi/linux/bpf_perf_event.h>

      const int max_cpus = 128;

      BPF_ARRAY(instructions, u64, max_cpus);
      BPF_ARRAY(cycles, u64, max_cpus);

      int on_cpu_instruction(struct bpf_perf_event_data *ctx) {
          instructions.increment(bpf_get_smp_processor_id(), ctx->sample_period);
          return 0;
      }

      int on_cpu_cycle(struct bpf_perf_event_data *ctx) {
          cycles.increment(bpf_get_smp_processor_id(), ctx->sample_period);
          return 0;
      }

  # See:
  # * https://github.com/iovisor/bcc/blob/master/tools/llcstat.py
  # * https://github.com/iovisor/bcc/blob/master/tools/llcstat_example.txt
  - name: llcstat
    metrics:
      counters:
        - name: llc_references_total
          help: Last level cache operations by type
          table: references
          labels:
            - name: cpu
              size: 4
              decoders:
                - name: uint
        - name: llc_misses_total
          help: Last level cache operations by type
          table: misses
          labels:
            - name: cpu
              size: 4
              decoders:
                - name: uint
    perf_events:
      - type: 0x0 # HARDWARE
        name: 0x3 # PERF_COUNT_HW_CACHE_MISSES
        target: on_cache_miss
        sample_frequency: 99
      - type: 0x0 # HARDWARE
        name: 0x2 # PERF_COUNT_HW_CACHE_REFERENCES
        target: on_cache_reference
        sample_frequency: 99
    code: |
      #include <linux/ptrace.h>
      #include <uapi/linux/bpf_perf_event.h>

      const int max_cpus = 128;

      BPF_ARRAY(references, u64, max_cpus);
      BPF_ARRAY(misses, u64, max_cpus);

      int on_cache_miss(struct bpf_perf_event_data *ctx) {
          misses.increment(bpf_get_smp_processor_id(), ctx->sample_period);
          return 0;
      }

      int on_cache_reference(struct bpf_perf_event_data *ctx) {
          references.increment(bpf_get_smp_processor_id(), ctx->sample_period);
          return 0;
      }

  - name: branchstat
    metrics:
      counters:
        - name: cpu_branch_instructions_total
          help: Branch instructions retired by CPUs
          table: instructions
          labels:
            - name: cpu
              size: 4
              decoders:
                - name: uint
        - name: cpu_branch_misses_total
          help: Mispredicted branch instructions
          table: misses
          labels:
            - name: cpu
              size: 4
              decoders:
                - name: uint
    perf_events:
      - type: 0x0 # HARDWARE
        name: 0x5 # PERF_COUNT_HW_BRANCH_MISSES
        target: on_branch_misses
        sample_frequency: 99
      - type: 0x0 # HARDWARE
        name: 0x4 # PERF_COUNT_HW_BRANCH_INSTRUCTIONS
        target: on_branch_instructions
        sample_frequency: 99
    code: |
      #include <linux/ptrace.h>
      #include <uapi/linux/bpf_perf_event.h>

      const int max_cpus = 128;

      BPF_ARRAY(misses, u64, max_cpus);
      BPF_ARRAY(instructions, u64, max_cpus);

      int on_branch_misses(struct bpf_perf_event_data *ctx) {
          misses.increment(bpf_get_smp_processor_id(), ctx->sample_period);
          return 0;
      }

      int on_branch_instructions(struct bpf_perf_event_data *ctx) {
          instructions.increment(bpf_get_smp_processor_id(), ctx->sample_period);
          return 0;
      }

  # Count file syscalls
  - name: filestat
    metrics:
      counters:
        - name: file_calls_total
          help: Calls to file related functions
          table: stats
          labels:
            - name: operation
              size: 4
              decoders:
                - name: uint
                - name: static_map
                  static_map:
                    1: CREATE
                    2: OPEN
                    3: OPENAT
                    4: CLOSE
                    5: FSYNC
                    6: READ
                    7: READV
                    8: PREADV
                    9: PREADV2
                    10: WRITE
                    11: WRITEV
                    12: PWRITEV
                    13: PWRITEV2
    tracepoints:
      syscalls:sys_enter_creat: tracepoint__syscalls__sys_enter_creat
      syscalls:sys_enter_open: tracepoint__syscalls__sys_enter_open
      syscalls:sys_enter_openat: tracepoint__syscalls__sys_enter_openat
      syscalls:sys_enter_close: tracepoint__syscalls__sys_enter_close
      syscalls:sys_enter_fsync: tracepoint__syscalls__sys_enter_fsync
      syscalls:sys_enter_read: tracepoint__syscalls__sys_enter_read
      syscalls:sys_enter_readv: tracepoint__syscalls__sys_enter_readv
      syscalls:sys_enter_preadv: tracepoint__syscalls__sys_enter_preadv
      syscalls:sys_enter_preadv2: tracepoint__syscalls__sys_enter_preadv2
      syscalls:sys_enter_write: tracepoint__syscalls__sys_enter_write
      syscalls:sys_enter_writev: tracepoint__syscalls__sys_enter_writev
      syscalls:sys_enter_pwritev: tracepoint__syscalls__sys_enter_pwritev
      syscalls:sys_enter_pwritev2: tracepoint__syscalls__sys_enter_pwritev2
    code: |
      #include <uapi/linux/ptrace.h>
      #include <uapi/linux/fs.h>

      typedef __kernel_rwf_t rwf_t;

      enum stat_types {
          S_CREATE = 1,
          S_OPEN,
          S_OPENAT,
          S_CLOSE,
          S_FSYNC,
          S_READ,
          S_READV,
          S_PREADV,
          S_PREADV2,
          S_WRITE,
          S_WRITEV,
          S_PWRITEV,
          S_PWRITEV2,
          S_MAXSTAT
      };

      BPF_ARRAY(stats, u64, S_MAXSTAT);

      static void stats_increment(int key) {
          u64 *leaf = stats.lookup(&key);
          if (leaf) (*leaf)++;
      }

      TRACEPOINT_PROBE(syscalls, sys_enter_creat) {
          stats_increment(S_CREATE);
          return 0;
      }
      TRACEPOINT_PROBE(syscalls, sys_enter_open) {
          stats_increment(S_OPEN);
          return 0;
      }
      TRACEPOINT_PROBE(syscalls, sys_enter_openat) {
          stats_increment(S_OPENAT);
          return 0;
      }
      TRACEPOINT_PROBE(syscalls, sys_enter_close) {
          stats_increment(S_CLOSE);
          return 0;
      }
      TRACEPOINT_PROBE(syscalls, sys_enter_fsync) {
          stats_increment(S_FSYNC);
          return 0;
      }
      TRACEPOINT_PROBE(syscalls, sys_enter_read) {
          stats_increment(S_READ);
          return 0;
      }
      TRACEPOINT_PROBE(syscalls, sys_enter_readv) {
          stats_increment(S_READV);
          return 0;
      }
      TRACEPOINT_PROBE(syscalls, sys_enter_preadv) {
          stats_increment(S_PREADV);
          return 0;
      }
      TRACEPOINT_PROBE(syscalls, sys_enter_preadv2) {
          stats_increment(S_PREADV2);
          return 0;
      }
      TRACEPOINT_PROBE(syscalls, sys_enter_write) {
          stats_increment(S_WRITE);
          return 0;
      }
      TRACEPOINT_PROBE(syscalls, sys_enter_writev) {
          stats_increment(S_WRITEV);
          return 0;
      }
      TRACEPOINT_PROBE(syscalls, sys_enter_pwritev) {
          stats_increment(S_PWRITEV);
          return 0;
      }
      TRACEPOINT_PROBE(syscalls, sys_enter_pwritev2) {
          stats_increment(S_PWRITEV2);
          return 0;
      }
